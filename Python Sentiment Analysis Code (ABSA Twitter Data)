# Analysis performed on jupyter notebook

# Import libraries
import re 
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
import numpy as np
import neattext as nt
import neattext.functions as nfx
import nltk
from nltk.tokenize import word_tokenize
nltk.download('wordnet')
nltk.download('punkt')
from nltk.stem import wordnet
from nltk.corpus import wordnet
nltk.download('averaged_perceptron_tagger')
from nltk.stem import WordNetLemmatizer as wordnet_lemmatizer
from nltk import pos_tag
from nltk.corpus import stopwords
nltk.download('stopwords')
stopwords.words('english')
import textblob
from textblob import TextBlob
import wordcloud
from wordcloud import WordCloud
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)


# Import the data set
absa_tweets = pd.read_csv("C:/Users/Siya/Documents/Quant Internship/Case Studies/Absa Case Study/ABSA Data.csv", sep = "\n", encoding = 'ISO-8859-1', header=None)


# Split and expand the dataframe
df = absa_tweets[0].str.split(',', expand=True)
df.to_csv('Absa_twitter_data.csv', index=False, header=None)

df = pd.read_csv('Absa_twitter_data.csv', low_memory=False)


# Feature extraction
# Isolate fields that contain 'Absa' and related terms in the 'extract' column
data = df.dropna(subset=['extract'], how='any') #Remove rows with with NA in the extract collumn
df = data.copy()
df = df[df['extract'].str.contains('ABSA|Absa|absa|cellphone banking|online banking|app|internet banking')]

# Isolate 'id' and 'extract' columns
df = df[['extract']]


# Data Cleaning
# Remove duplicates and NaN values
df = df.drop_duplicates(keep='first')
df = df.dropna() 


# Isolate fields that contain 'Absa' in the 'extract' column
df['clean_tweets'] = df['extract'].apply(nfx.remove_userhandles)
df['clean_tweets'] = df['clean_tweets'].apply(nfx.remove_hashtags)
df['clean_tweets'] = df['clean_tweets'].apply(lambda x: nfx.remove_custom_pattern(x, term_pattern = r'RT'))
df['clean_tweets'] = df['clean_tweets'].replace(to_replace=r'https?:\/\/.*[\r\n]*',value='',regex=True)
df['clean_tweets'].apply(nfx.remove_puncts)
df['clean_tweets'].apply(nfx.fix_contractions)
df['clean_tweets'] = df['clean_tweets'].apply(nfx.remove_special_characters)
df['clean_tweets'] = df['clean_tweets'].apply(nfx.remove_numbers)
df['clean_tweets'] = df['clean_tweets'].apply(lambda x: nt.TextFrame(x).remove_stopwords())

df.drop(['extract'], axis='columns', inplace=True)


# POS tagger dictionary
pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}
def token_stop_pos(text):
    tags = pos_tag(word_tokenize(text))
    newlist = []
    for word, tag in tags:
        if word.lower() not in set(stopwords.words('english')):
            newlist.append(tuple([word, pos_dict.get(tag[0])]))
    return newlist
df['clean_tweets'] = df['clean_tweets'].apply(str)
df['POS tagged'] = df['clean_tweets'].apply(token_stop_pos)


def lemmatize(pos_data):
    lemma_rew = " "
    for word, pos in pos_data:
        if not pos:
            lemma = word
            lemma_rew = lemma_rew + " " + lemma
        else:
            lemma = wordnet_lemmatizer().lemmatize(word, pos=pos)
            lemma_rew = lemma_rew + " " + lemma
    return lemma_rew

df['lemma'] = df['POS tagged'].apply(lemmatize)


# Common word visualization
all_words = ' '.join([tweets for tweets in df['clean_tweets']])
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)
plt.figure(figsize=(10, 7))
plt.title('All Tweets (First Cloud)')
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()


# Delete rows that contain words unrelated to Absa customer sentiment analysis based on above visualization
df = df[~df.lemma.str.contains('lockdown')]
df = df[~df.lemma.str.contains('Premier|premier|Premiership|premier league|league champion')]
df = df[~df.lemma.str.contains('Chiefs|Kaizer Chiefs|CHIEFS')]
df = df[~df.lemma.str.contains('Orlando Pirates')]
df = df[~df.lemma.str.contains('FNB|fnb|Fnb')]
df = df[~df.lemma.str.contains('Capitec|capitec|CAPITEC')]
df = df[~df.lemma.str.contains('Standard Bank|Standard BankZA|standard bank|standard bankZA')]
df = df[~df.lemma.str.contains('Peter Mokaba|Mokaba Stadium')]
df = df[~df.lemma.str.contains('Covid|COVID|covid|Flulike symtoms|symptoms|health department')]
df = df[~df.lemma.str.contains('Cyrill Ramaphosa|CR Cyril|CYRIL')]
df = df[~df.lemma.str.contains('Adv Mkhwebane')]
df = df[~df.lemma.str.contains('South Africa')]
df = df[~df.lemma.str.contains('Jele Itumeleng')]

# Run wordcloud plot again to confirm deletion of unrelated words
all_words = ' '.join([tweets for tweets in df['clean_tweets']])
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)
plt.figure(figsize=(10, 7))
plt.title('All Tweets (Second Cloud)')
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()


# Sentiment analysis

# Create functions to get subjectivity and polarity
def getSubjectivity(text):
    return TextBlob(text).sentiment.subjectivity

def getPolarity(text):
    return TextBlob(text).sentiment.polarity


# Create subjectivity and polarity columns
df['subjectivity'] = df['clean_tweets'].apply(getSubjectivity)
df['polarity'] = df['clean_tweets'].apply(getPolarity)


# Create fuction to compute the negative, neutral, and postive analysis
def getAnalysis(score):
    if score < 0:
        return 'Negative'
    elif score == 0:
        return 'Neutral'
    else:
        return 'Postive'
        
df['analysis'] = df['polarity'].apply(getAnalysis)


# compute the percentage of positve, negative and neutral tweets
tweet_pcnt = round(df['analysis'].value_counts()/df.shape[0]*100,1)
ntweets = tweet_pcnt[0] #ntweets = neutral tweets
ptweets = tweet_pcnt[1] #ptweets = positive tweets
etweets = tweet_pcnt[2] #etweets = negative tweets


# Create bar graph to visualize the sentiments
plt.title('Absa Twitter Sentiment Analysis')
plt.xlabel('Sentiment')
plt.ylabel('Counts')

# Create the bar plot
ax = df['analysis'].value_counts().plot(kind='bar')

# Add labels to each bar
for p in ax.patches:
    ax.annotate(str(p.get_height()), (p.get_x() + p.get_width()/2, p.get_height()), ha = 'center')

plt.show()

# Create pie chart
explode = (0.1,0.1,0.1)
labels = 'Postive', 'Neutral', 'Negative'
sizes = [ptweets, ntweets, etweets]
colors = ['yellowgreen', 'lightcoral', 'gold']

plt.pie(sizes, labels = labels, explode=explode, colors=colors, autopct = '%1.1f%%', startangle = 120)
plt.title('ABSA Twitter Sentiment Analysis')
plt.axis('equal')
plt.show()

# Create word cloud to visualize words used in negative tweets
data_neg = " ".join(df[df['analysis'] =='Negative']['lemma'])
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(data_neg)
plt.figure(figsize = (10,7))
plt.title('Negative Tweets')
plt.imshow(wordcloud, interpolation = 'bilinear')
plt.axis('off')
plt.show()
